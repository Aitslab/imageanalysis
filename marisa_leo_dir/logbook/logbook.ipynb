{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1599a432",
   "metadata": {},
   "source": [
    "# Logbook\n",
    "### Leo Westerberg & Marisa Wodrich\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2c031",
   "metadata": {},
   "source": [
    "\n",
    "#### Date: 5 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "To understand how the hoVer-Net is built and what part of the github code belongs to what in the architecture\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Read the papers about HoVer-Net to truly understand its concept\n",
    "* Try to extract code which belongs to each part of the model code\n",
    "* Try to filter out the part which belongs to classification\n",
    "* Create a notebook for example code comment your results\n",
    "\n",
    "#### Result/Conclusion: \n",
    "After having a hard time understanding the HoVer-Net and its underlying structure, https://www.sciencedirect.com/science/article/abs/pii/S1361841519301045?via%3Dihub, I found a great video which explains the concepts thoroughly: https://www.youtube.com/watch?v=PfDauqZdUCE and now I understand each part of the architecture and what its benefits are.\n",
    "\n",
    "Going through the code in the HoVer-Net github repositoty, was great for getting some connection between intuition and code and I created some trivial comments in a newly created notebook. \n",
    "https://github.com/Aitslab/imageanalysis/blob/master/marisa_leo_dir/example_code.ipynb\n",
    "\n",
    "#### Next steps: \n",
    "* Get a deeper understanding how input is passed into the model\n",
    "* Understand how to use Cvat\n",
    "* Annotate some images\n",
    "* Setting up our own segmentation model based on code in the ***example_code.ipynb*** notebook\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947de287",
   "metadata": {},
   "source": [
    "\n",
    "#### Date: 6 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Understand the imput to the HoVer-Net and then try to run the HoVer-Net on Google colab to better understand its underlying structure with the example data\n",
    "#### Stepwise description of work:\n",
    "* Browse the code and watch the input data\n",
    "* Clone the project att step by step transfer parts to google colab\n",
    "\n",
    "\n",
    "#### Result/Conclusion: \n",
    "The result of the day was that we got a better understanding of the data passed into the HoVer-Net. We also tried to run the project on our own laptops with the example data. After bumping into several import errors and configuration differences, we tried to upload the project to Google colab but came across many hard bugs and errors when trying the copy and paste code into the notebook partwise.\n",
    "#### Next steps: \n",
    "* Continue to setup the project on Google colab\n",
    "* Annotate some images\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f15926",
   "metadata": {},
   "source": [
    "\n",
    "#### Date: 8 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Try to get HoverNet or MicroNet to work on my machine also setting upp the report document\n",
    "\n",
    "\n",
    "#### Stepwise description of work:\n",
    "\n",
    "\n",
    "* Try to fix the error of:\n",
    "2022-04-08 22:32:47.231415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
    "\n",
    "AND\n",
    "\n",
    "File \"src/train.py\", line 270, in <module>\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "  File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/os.py\", line 674, in __setitem__\n",
    "    value = self.encodevalue(value)\n",
    "  File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/os.py\", line 744, in encode\n",
    "    raise TypeError(\"str expected, not %s\" % type(value).__name__)\n",
    "TypeError: str expected, not NoneType\n",
    "\n",
    "\n",
    "* Creating wordfile with IEEE template and share with supervisors\n",
    "#### Result/Conclusion: \n",
    "Finally the first error was solved with the following commands in a specific conda enviroment\n",
    "\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
    "sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb\n",
    "sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb\n",
    "sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda\n",
    "\n",
    "No progress on solving the second error and I am wondering if it is a machine specific error, but since it does not work on google colab, which have a GPU, it am not certain.\n",
    "\n",
    "I created a word file and found a IEEE template which can be converted to latex later\n",
    "\n",
    "#### Next steps: \n",
    "* Try to solve the second error. If still stuck, try to run on the university system\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c6e9a",
   "metadata": {},
   "source": [
    "#### Date: 9 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "* Try to get HoverNet on work on own machine or Google colab\n",
    "\n",
    "* Try to fix the error of:\n",
    "    File \"src/train.py\", line 270, in <module>\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "      File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/os.py\", line 674, in __setitem__\n",
    "        value = self.encodevalue(value)\n",
    "      File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/os.py\", line 744, in encode\n",
    "        raise TypeError(\"str expected, not %s\" % type(value).__name__)\n",
    "    TypeError: str expected, not NoneType\n",
    "\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Try different compatible versions of cudatoolkit and cudnn to fix yesterdays error\n",
    "* complete settings in opt/hover with pretrained set\n",
    "* debug errors in hovernet code\n",
    "\n",
    "\n",
    "\n",
    "#### Result/Conclusion: \n",
    "Yesterday I thought that I had solved the problem with:\n",
    "\n",
    " Try to fix the error of:\n",
    "    2022-04-08 22:32:47.231415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
    "\n",
    "However, this was not the case and the error was shadowed by another one. Starting from the beginning I tried different conda enviroments with different settings and concludingly the commands nessesary for setting up the right enviroment is\n",
    "    conda create --name hovernet python=3.6\n",
    "    conda activate hovernet\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "where requirements.txt consists of:\n",
    "\n",
    "    tensorflow-gpu==1.12.0\n",
    "    tensorpack==0.9.0.1\n",
    "    scikit-image==0.14.2\n",
    "    matplotlib===3.0.2\n",
    "    numpy==1.15.4\n",
    "    scipy==1.4.1\n",
    "    opencv-python==4.1.2.30\n",
    "    openslide-python==1.1.2\n",
    "\n",
    "    conda install cudatoolkit=9.0.0\n",
    "\n",
    "The next problem was the error\n",
    "\n",
    "    File \"src/train.py\", line 270, in <module>\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "      File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/os.py\", line 674, in __setitem__\n",
    "        value = self.encodevalue(value)\n",
    "      File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/os.py\", line 744, in encode\n",
    "        raise TypeError(\"str expected, not %s\" % type(value).__name__)\n",
    "    TypeError: str expected, not NoneType\n",
    "\n",
    "which I realized was due to not setting the right arguments to the run command. Initially we ran it with:\n",
    "    python3 train.py\n",
    "But it as missing the --gpu='...' argument. For example\n",
    "    python train.py --gpu='0'\n",
    "Where the 0 represent a gpu index\n",
    "\n",
    "\n",
    "We also had to download the pretrained set according to the README in the github repository and place the relative path in the opt/hover.py\n",
    "\n",
    "Now we are stuck with an error\n",
    "\n",
    "(hovernet) leo@leo:~/Skrivbord/hover_net/src$ python train.py --gpu='0'\n",
    "[270, 270] [80, 80]\n",
    "Traceback (most recent call last):\n",
    "  File \"train.py\", line 272, in <module>\n",
    "    trainer.run()\n",
    "  File \"train.py\", line 240, in run\n",
    "    self.run_once(opt, sess_init=init_weights, save_dir=log_dir)\n",
    "  File \"train.py\", line 174, in run_once\n",
    "    train_datagen = self.get_datagen(opt['train_batch_size'], mode='train')\n",
    "  File \"train.py\", line 162, in get_datagen\n",
    "    nr_procs=nr_procs)\n",
    "  File \"/home/leo/Skrivbord/hover_net/src/loader/loader.py\", line 72, in train_generator\n",
    "    ds = BatchDataByShape(ds, batch_size, idx=0)\n",
    "  File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 179, in __init__\n",
    "    super(BatchDataByShape, self).__init__(ds, batch_size, remainder=False)\n",
    "  File \"/home/leo/anaconda3/envs/hovernet/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 95, in __init__\n",
    "    assert batch_size <= len(ds)\n",
    "AssertionError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Next steps: \n",
    "* Try to solve the new error showed above\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d1f8f",
   "metadata": {},
   "source": [
    "#### Date: 11 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "* Try the micronet with the newly installed conda enviroment\n",
    "* Write about different metrics to evaluate the segmentation model\n",
    "\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Try the micronet with the new versions installed, refer to setup steps from 8th april.\n",
    "* Go through malous report internet to search for different metrics\n",
    "* Write brefly about these metrics\n",
    "\n",
    "\n",
    "#### Result/Conclusion: \n",
    "When trying the micronet using its Demo.ipynb jupyter notebook we bump into the error:\n",
    "\n",
    "    AssertionError                            Traceback (most recent call last)\n",
    "    <ipython-input-3-645e4c6e65b9> in <module>()\n",
    "          3     out_file = out_dir + img_path.split('/')[-2]\n",
    "          4     out_path = out_file+'/'+img_name+'.mat'\n",
    "    ----> 5     pred_map = inferer.run(img_path)\n",
    "          6     pred_inst, pred_type = process_instance_micro(pred_map, inferer.nr_types)\n",
    "          7     labels={'Instance':pred_inst,'Type':pred_type}\n",
    "\n",
    "    /home/leo/Skrivbord/Instance_based_Vision_Transformer/nuclei_seg_cls_infer/single_image_infer.py in run(self, file_path)\n",
    "        271         model_constructor = self.get_model()\n",
    "        272         pred_config = PredictConfig(\n",
    "    --> 273             model        = model_constructor(),\n",
    "        274             session_init = get_model_loader(model_path),\n",
    "        275             input_names  = self.eval_inf_input_tensor_names,\n",
    "\n",
    "    /home/leo/Skrivbord/Instance_based_Vision_Transformer/nuclei_seg_cls_infer/micronet.py in __init__(self)\n",
    "        19     def __init__(self):\n",
    "        20         super(Graph, self).__init__()\n",
    "    ---> 21         assert tf.test.is_gpu_available()\n",
    "        22         self.data_format = 'channels_first'\n",
    "        23 \n",
    "\n",
    "    AssertionError: \n",
    "\n",
    "This is caused by the laptop used not having a dedicated gpu and since this is an error from the last cell of the notebook I assume it should be working fine if it was connected to a GPU. For the hovernet, the status is the same as 9th april, but we will try to run that from a laptop with a GPU as well to see if we proceed further.\n",
    "\n",
    "Looking into different metrics, the report is filled with breif information of Jaccard index, recall, precision and f1 score together with links to sources. For more info, look at the report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Next steps: \n",
    "* Try the micronet on a computer with a cuda compatible gpu\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6e918",
   "metadata": {},
   "source": [
    "#### Date: 17 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Try the micronet on Google colab\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Experiment with training the micronet on Google colab\n",
    "* Having the demo.ipynb as entry point, add needed code and folders\n",
    "\n",
    "#### Result/Conclusion: \n",
    "\n",
    "Today, we experimented on trying the micronet on google collab. The progress was pretty good until we bumped into the same error as Leo did on his personal laptop\n",
    "\n",
    "        AssertionError                            Traceback (most recent call last)\n",
    "        <ipython-input-33-1a42ad9e7890> in <module>()\n",
    "            4     out_path = out_file+'/'+img_name+'.mat'\n",
    "            5     print(img_path)\n",
    "        ----> 6     pred_map = inferer.run(img_path)\n",
    "            7     pred_inst, pred_type = process_instance_micro(pred_map, inferer.nr_types)\n",
    "            8     labels={'Instance':pred_inst,'Type':pred_type}\n",
    "\n",
    "        1 frames\n",
    "        <ipython-input-28-80ce3c1597bd> in run(self, file_path)\n",
    "            268         model_constructor = self.get_model()\n",
    "            269         pred_config = PredictConfig(\n",
    "        --> 270             model        = model_constructor(),\n",
    "            271             session_init = get_model_loader(model_path),\n",
    "            272             input_names  = self.eval_inf_input_tensor_names,\n",
    "\n",
    "        <ipython-input-25-096bf8af8f19> in __init__(self)\n",
    "            112             def __init__(self):\n",
    "            113                 super(Graph, self).__init__()\n",
    "        --> 114                 assert tf.test.is_gpu_available()\n",
    "            115                 self.data_format = 'channels_first'\n",
    "            116 \n",
    "\n",
    "        AssertionError: \n",
    "\n",
    "Knowing we have a GPU running on google colab, it assertion says that is is not available... \n",
    "\n",
    "\n",
    "#### Next steps: \n",
    "* Try to fix the issue and continue\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cbb9f",
   "metadata": {},
   "source": [
    "#### Date: 20 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Set up the enviroment on a GPU compatible device\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Try doing the same steps as mensioned on 8th april.\n",
    "* Export the enviroment settings file\n",
    "* Try fixing the problem of Tensorflow not resolving\n",
    "* Try another fresh attempt on Google colab.\n",
    "\n",
    "#### Result/Conclusion: \n",
    "\n",
    "Since Leo's laptop has the correct dependency configuration, the next step was to transfer this enviroment to Marisa's Laptop, having a GPU. Marisa have bumped into loads of problems trying to set up the enviroment and therefore we tried to do the same steps as we did on Leo's computer. Weirdly, Tensorflow-gpu and Cudatoolkit is installed, with the same versions working on leos laptop, \"Tensorflow as tf\" can not be resolved. We tried different pure \"tensorflow\" versions but that only messed up other dependencies. \n",
    "\n",
    "After some research it is a problem sometimes occuring with not recognizing cudatoolkit, making the tensorflow-gpu package visible. But this is weird since cudatoolkit, version 9.0, was in the installed packages.\n",
    "\n",
    "The next appempt was to make a transfer of Leo's enviroment through an .yml enviroment file. However, the same problems with Tensorflow now resolving occured. We wondered if it maybe had to do with the base configuration on Leo's laptop. But browsing through the installed packages, the pure tensorflow version where not there either, so we dispanded that hypothesis.\n",
    "\n",
    "After hours of trying similar things and different versions, we decided to try another attempt at the hovernet on Google colab.\n",
    "\n",
    "However, after studying the segmentation model into detail we discovered that it will not be enought computing power on either Marisa's laptop or Colab. Therefore, so we do not to double work, after this attempt with google colab, the next step is to set up the enviroment on a system with guarenteed computing power, like the university system.\n",
    "\n",
    "#### Next steps: \n",
    "* If no progress on the fresh colab, try the micronet on a university computer with a cuda compatible GPU\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba01985",
   "metadata": {},
   "source": [
    "#### Date: 21 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Understand the HoVer-Net better by building it by our own\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Continue with the Colab\n",
    "* Compare with original HoVer-Net paper, architecture graphs, as well as with the original implementation\n",
    "\n",
    "#### Result/Conclusion: \n",
    "\n",
    "The network is really big. We did not finish the own implementation yet. The encoder part is done, however, some questions came up during the implementation. \n",
    "\n",
    "In the paper, they write that they use skip connections and instead of concatenating old feature maps (this is what the skip connection does) with new ones, they use summation. This is not further explained and is very confusing. Summation does not make sense, since the shapes are changed and therefore do not match anymore. In the original code, they indeed use summation, but I do not understand why this works. \n",
    "\n",
    "#### Next steps: \n",
    "* Figure out the summation part and the skip connections\n",
    "* Finish the decoder branches (should be really quick to code)\n",
    "* Code the preprocessing. This is not fully explained in the paper, but we have the original code as a reference\n",
    "* Code the training\n",
    "* Try the original code maybe on a cluster. Hopefully we can get it to run somewhere. But in any case, coding it once on our own will help a lot with understanding the HoVer-Net.\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76680416",
   "metadata": {},
   "source": [
    "#### Date: 22 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Get the micronet interference to work\n",
    "\n",
    "#### Stepwise description of work:\n",
    "* Debug the google colab document and fix paths\n",
    "#### Result/Conclusion: \n",
    "\n",
    "Since running and solving the problems are described with details in the google colab document, I refer to the markdown notes in the google colab document\n",
    "https://colab.research.google.com/drive/1ydyXU6iu2SXuwCOtUdVle3gxR3x_lH-S?usp=sharing\n",
    "#### Next steps: \n",
    "* Try to get the micronet training implementation in the github repo https://github.com/vqdang/hover_net/blob/tensorflow-final/ to work\n",
    "* Use the trained model and plug it in into https://colab.research.google.com/drive/1ydyXU6iu2SXuwCOtUdVle3gxR3x_lH-S?usp=sharing\n",
    "using a weight file\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928c6d0",
   "metadata": {},
   "source": [
    "#### Date: 26 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Get introduced into the cluster and learn more about tensorflow for better understanding the  https://github.com/vqdang/hover_net/blob/tensorflow-final/\n",
    "#### Stepwise description of work:\n",
    "* Watch videos on youtube for understanding tensorflow\n",
    "* Have an introduction on the berzelius cluster\n",
    "#### Result/Conclusion: \n",
    "The day began with spending some hours on watching https://www.youtube.com/watch?v=tPYj3fFJGjk. The video was very detailed and therefore it is now much easier to undestand the source code of the github project\n",
    "\n",
    "Then we met up with Salma to discuss further steps, but also to get an introduction to the clusters.\n",
    "#### Next steps: \n",
    "* Set up the enviroment on the cluster and then use the code from the github repo to train a model using the micronet setting in the config.py file\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf29335",
   "metadata": {},
   "source": [
    "#### Date: 27 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Set up the clusterenviroment on the cluster and begin working with the hovernet\n",
    "#### Result/Conclusion: \n",
    "First of all we created an account on berzelious super computer which took a while to be validated. Then we tried to do the steps described on 9th april to create a conda enviroment, but it bumped into problems configuring the cudnn. Then after some attepmts on different versions a the general command:\n",
    "       \n",
    "        conda install -c conda-forge cudnn\n",
    "\n",
    "worked in addition to the commands on 9th april. This worked very well and the enviroment is now working with the hovernet. Then we sat down trying to get the inference to work on the pictures given by Sonja. We bumped into some problems with a non excisting log file and we realized that some requirements for using the network is to have files and modifications that are created during training (Excluding the model files).\n",
    "\n",
    "However when we tried to experiment with training, we realized that the format of our own training material was in the wrong format. Tried quickly different conversions, without success and decided to instead try to train the network with the example data and then infer some pictures of that trained network. If that is successfull we know that all the code works and the only thing that we need to change is the format of the input data (time management)\n",
    "\n",
    "#### Next steps: \n",
    "* Try to train the network on the cluster using their data\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bfcf2",
   "metadata": {},
   "source": [
    "#### Date: 28 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Train the network on the cluster\n",
    "#### Result/Conclusion: \n",
    "\n",
    "After setting up the data as in described in the README files in https://github.com/vqdang/hover_net/blob/tensorflow-final/ & https://github.com/vqdang/hover_net/blob/tensorflow-final/src we extracted the patches using extract_patches.py. Here we also had to modify some configuration paths to fit accordingly to our folder structure. For example change the folder path \"/logs.json\" to \"logs.json\" since \"/\" means root folder in linux. After some more minor error fixing, much alike the one we had when setting up the project on Leo's computer the trair.py file successfully runs. However when we tried to run the program on the cluster using the command.\n",
    "Then for running the program on the cluster, we have to upgrade the cudnn and cuda versions using the command\n",
    "\n",
    "        conda install pytorch torchvision torchaudio cudatoolkit -c pytorch -c nvidia\n",
    "\n",
    "But that command interfere with program and will therefore be the next step to solve\n",
    "\n",
    "#### Next steps: \n",
    "\n",
    "* Make the training work on the cluster by finding compatible cuda/cudnn versions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae80a1c",
   "metadata": {},
   "source": [
    "#### Date: 2/3 april\n",
    "\n",
    "#### Project: \n",
    "Image analysis for high-throughput microscopy screening\n",
    "\n",
    "#### Aim: \n",
    "Train the model on the cluster using the example pictures\n",
    "#### Result/Conclusion: \n",
    "\n",
    "After spending some time trying to understand why the training get locked on epoch 1, we realized that all the GPUS allocated are overloaded for some reason. This is weird since using the cluster front end, which has a RTX quadro allocated to it works like a charm. Also the gpus, both on the cluster and the front end, are using cuda version 10.2 which makes us believe it has something to do with tensorflow and not the cuda versions. Using the front end there is no loading of the gpus while \n",
    "\n",
    "FRONT END (MUCH FASTER, estimated 4 HOUR FOR ALL 50 epochs to run)\n",
    "\n",
    "        [0504 10:59:39 @logger.py:73] Argv: train.py --gpu=0\n",
    "        2022-05-04 10:59:39.218854: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
    "        2022-05-04 10:59:39.379238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
    "        name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.62\n",
    "        pciBusID: 0000:01:00.0\n",
    "        totalMemory: 22.17GiB freeMemory: 20.76GiB\n",
    "        2022-05-04 10:59:39.379287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
    "        2022-05-04 10:59:40.754017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
    "        2022-05-04 10:59:40.754081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
    "        2022-05-04 10:59:40.754088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
    "        2022-05-04 10:59:40.754277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 20113 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
    "        [0504 10:59:40 @interface.py:34] Automatically applying QueueInput on the DataFlow.\n",
    "\n",
    "\n",
    "BACK END (FREEZEZ AT EPOCH 1)\n",
    "        [270, 270] [80, 80]\n",
    "        \u001b[32m[0504 11:01:49 @parallel.py:293]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
    "        [270, 270] [80, 80]\n",
    "        \u001b[32m[0504 11:01:49 @logger.py:73]\u001b[0m Argv: train.py --gpu=0\n",
    "        2022-05-04 11:01:49.367183: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
    "        2022-05-04 11:01:49.931788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
    "        name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
    "        pciBusID: 0000:4e:00.0\n",
    "        totalMemory: 39.59GiB freeMemory: 39.18GiB\n",
    "        2022-05-04 11:01:49.931861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
    "        tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
    "        2022-05-04 11:03:51.632320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
    "        2022-05-04 11:03:51.633060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
    "        2022-05-04 11:03:51.633075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
    "        2022-05-04 11:03:51.633258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 38034 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4e:00.0, compute capability: 8.0)\n",
    "        \u001b[32m[0504 11:03:51 @interface.py:34]\u001b[0m Automatically applying QueueInput on the DataFlow.\n",
    "\n",
    "\n",
    "Since it is perfered to run it on the back end we tried different methods to decrease the load(which is weird since the single quadro gpu works like a charm), decreasing the batch size did not work, neither allocating up to 10 gpus(ALL WHERE EXTREMELY OVERLOADED, THINK THERE IS DUE TO AN ERROR AND NOT TO THE CODE). \n",
    "\n",
    "        Traceback (most recent call last): File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call return fn(*args) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn options, feed_dict, fetch_list, target_list, run_metadata) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun run_metadata) tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(3686400, 1), b.shape=(1, 4), m=3686400, n=4, k=1 [[{{node tower1/hv/u2/rz/Tensordot/MatMul}} = MatMul[T=DT_FLOAT, _class=[\"loc:@tower1/gradients/tower1/hv/u2/rz/Tensordot/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](tower1/hv/u2/rz/Tensordot/Reshape, tower1/hv/u1/rz/Tensordot/Reshape_1)]] [[{{node tower1/np/u1/rz/Tensordot/Shape/_1463}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_6505_tower1/np/u1/rz/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]] During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"train.py\", line 272, in <module> trainer.run() File \"train.py\", line 240, in run self.run_once(opt, sess_init=init_weights, save_dir=log_dir) File \"train.py\", line 212, in run_once launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_gpus)) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/interface.py\", line 97, in launch_train_with_config extra_callbacks=config.extra_callbacks) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/base.py\", line 341, in train_with_defaults steps_per_epoch, starting_epoch, max_epoch) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/base.py\", line 313, in train self.main_loop(steps_per_epoch, starting_epoch, max_epoch) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper return func(*args, **kwargs) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/base.py\", line 278, in main_loop self.run_step() # implemented by subclass File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/base.py\", line 181, in run_step self.hooked_sess.run(self.train_op) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run run_metadata=run_metadata) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run run_metadata=run_metadata) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run raise six.reraise(*original_exc_info) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/six.py\", line 719, in reraise raise value File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run return self._sess.run(*args, **kwargs) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1312, in run run_metadata=run_metadata) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run return self._sess.run(*args, **kwargs) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run run_metadata_ptr) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run feed_dict_tensor, options, run_metadata) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run run_metadata) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(3686400, 1), b.shape=(1, 4), m=3686400, n=4, k=1 [[node tower1/hv/u2/rz/Tensordot/MatMul (defined at /home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/models/pool.py:131) = MatMul[T=DT_FLOAT, _class=[\"loc:@tower1/gradients/tower1/hv/u2/rz/Tensordot/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](tower1/hv/u2/rz/Tensordot/Reshape, tower1/hv/u1/rz/Tensordot/Reshape_1)]] [[{{node tower1/np/u1/rz/Tensordot/Shape/_1463}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_6505_tower1/np/u1/rz/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]] Caused by op 'tower1/hv/u2/rz/Tensordot/MatMul', defined at: File \"train.py\", line 272, in <module> trainer.run() File \"train.py\", line 240, in run self.run_once(opt, sess_init=init_weights, save_dir=log_dir) File \"train.py\", line 212, in run_once launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_gpus)) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/interface.py\", line 87, in launch_train_with_config model._build_graph_get_cost, model.get_optimizer) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper return func(*args, **kwargs) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 204, in setup_graph train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/trainers.py\", line 106, in _setup_graph self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 161, in build grad_list = DataParallelBuilder.build_on_towers(self.towers, get_grad_fn, devices) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 119, in build_on_towers ret.append(func()) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 232, in get_grad_fn cost = get_cost_fn(*input.get_input_tensors()) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/tfutils/tower.py\", line 284, in _call_ output = self._tower_fn(*args) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/graph_builder/model_desc.py\", line 246, in _build_graph_get_cost ret = self.build_graph(*inputs) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/graph_builder/model_desc.py\", line 162, in build_graph return self._build_graph(args) File \"/proj/berzelius-2021-21/users/leo-marisa-dir/hover_net/src/model/graph.py\", line 178, in _build_graph hv_feat = decoder('hv', d) File \"/proj/berzelius-2021-21/users/leo-marisa-dir/hover_net/src/model/graph.py\", line 94, in decoder u2 = upsample2x('rz', u3) File \"/proj/berzelius-2021-21/users/leo-marisa-dir/hover_net/src/model/graph.py\", line 24, in upsample2x data_format='channels_first') File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/models/registry.py\", line 124, in wrapped_func outputs = func(*args, **actual_args) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/models/pool.py\", line 131, in FixedUnPooling ret = tf.tensordot(x, mat, axes=1) # bxcxhxwxshxsw File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2985, in tensordot ab_matmul = matmul(a_reshape, b_reshape) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2057, in matmul a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4560, in mat_mul name=name) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper op_def=op_def) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func return func(*args, **kwargs) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op op_def=op_def) File \"/home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in _init_ self._traceback = tf_stack.extract_stack() InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(3686400, 1), b.shape=(1, 4), m=3686400, n=4, k=1 [[node tower1/hv/u2/rz/Tensordot/MatMul (defined at /home/x_lewes/.conda/envs/hovernet/lib/python3.6/site-packages/tensorpack/models/pool.py:131) = MatMul[T=DT_FLOAT, _class=[\"loc:@tower1/gradients/tower1/hv/u2/rz/Tensordot/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](tower1/hv/u2/rz/Tensordot/Reshape, tower1/hv/u1/rz/Tensordot/Reshape_1)]] [[{{node tower1/np/u1/rz/Tensordot/Shape/_1463}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_6505_tower1/np/u1/rz/Tensordot/Shape\", tensor_type=DT_INT32,\n",
    "\n",
    "Then we switched to interactive mode and it had the same error as with the back end(seems like it is using the same gpus as with the back end). \n",
    "\n",
    "We are wondering if we are allowed to just use the front end since it works so good compared to the back end. Later, we started working on converting Malous pictures into the same format as the example pictures in Matlab\n",
    "\n",
    "\n",
    "\n",
    "#### Next steps: \n",
    "\n",
    "* Make the training work on the cluster by finding compatible cuda/cudnn versions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
